###### Narendara Modi Tweet Analysis (2014-2018) #####

tweets=read.csv("2014_2018_Tweets.csv",quote = "",header = T, sep = ",",
                row.names = NULL, 
                stringsAsFactors = FALSE)
View(tweets)

str(tweets)
typeof(tweets$Tweet.Text)
length(tweets$Tweet.Text)
# Build Corpus
library(wordcloud)
library(tm)
corpus=iconv(tweets$Tweet.Text,to ="latin1")     # Character conversion
typeof(corpus)
corpus=Corpus(VectorSource(corpus))
inspect(corpus[1:5])

# Clean text
corpus=tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus=tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus=tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
cleanset=tm_map(corpus,removeWords,stopwords('english'))
inspect(cleanset[1:5])
cleanset=tm_map(cleanset,removeWords, c('and','the','our','that','for','are','also','more','has','must','have','should','this','with','will','spoke'))
inspect(cleanset[1:5])
removeURL=function(x) gsub('http[[:alnum:]]*','',x)
cleanset=tm_map(cleanset,gsub,pattern='gujarats',replacement='gujarat')
cleanset <- tm_map(cleanset, stemDocument)
cleanset=tm_map(cleanset,content_transformer(removeURL))
inspect(cleanset[1:5])
cleanset=tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])

# Term Document Matrix
tdm=TermDocumentMatrix(cleanset)
tdm
tdm=as.matrix(tdm)
tdm[1:10,1:20]

# BArplot
w=rowSums(tdm)
w
w=subset(w,w>=100)
w

typeof(w)
class(w)
p=sort(w,decreasing = T)
p


barplot(p,las=2,col=rainbow(50))

names(w)
typeof(w)




# Word Cloud
library(wordcloud)
w=sort(rowSums(tdm),decreasing = T)
View(w)

set.seed(200)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8,'Dark2'),
          rot.per = 0.3)

library(wordcloud2)
w=data.frame(names(w),w)
colnames(w)=c('word','freq')
w1=as.data.frame(w)
wordfreq.csv=write.csv(w1)
write.table(w1, file="wordfreq_Stemd.csv", row.names=FALSE, col.names=TRUE, sep=",", dec=".", quote=TRUE)

View(wordfreq.csv)
wordcloud2(w,
           size = 0.5,
           shape='circle',
           rotateRatio = 0.5,
           minSize = 1)


# Sentiment Analysis
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)

# Read file
tweets=read.csv("2014_2018_Tweets.csv",quote = "",header = T, sep = ",",
                row.names = NULL, 
                stringsAsFactors = FALSE)
corpus=iconv(tweets$Tweet.Text,to ="latin1")

# Obtain Sentimental Scores
s=get_nrc_sentiment(corpus)
head(s)

s
ang=s$anger
sum(ang)
anti=s$anticipation
sum(anti)
dis=s$disgust
sum(dis)
fear=s$fear
sum(fear)
joy=s$joy
sum(joy)
sad=s$sadness
sum(sad)
sur=s$surprise
sum(sur)
trus=s$trust
sum(trus)
neg=s$negative
sum(neg)
pos=s$positive
sum(pos)
length(s$anger)
length(s$anticipation)
length(s$disgust)
length(s$fear)
length(s$joy)
length(s$sadness)
length(s$surprise)
length(s$trust)
length(s$negative)
length(s$positive)



corpus[4]
get_nrc_sentiment("create")

# Bar Plot
barplot(colSums(s),
        las=2,
        col=rainbow(10),
        ylab = 'Count',
        main = 'Sentimental Scores of NM Tweets from 2014 to 2018')


